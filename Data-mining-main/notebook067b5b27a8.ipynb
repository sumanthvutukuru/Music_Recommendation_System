{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom fuzzywuzzy import fuzz\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.sparse import csr_matrix\n\n\nclass Recommender:\n    def __init__(self, metric, algorithm, k, data, decode_id_song):\n        self.metric = metric\n        self.algorithm = algorithm\n        self.k = k\n        self.data = data\n        self.decode_id_song = decode_id_song\n        self.data = data\n        self.model = self._recommender().fit(data)\n\n    def make_recommendation(self, new_song, n_recommendations):\n        recommended = self._recommend(new_song=new_song,\n                                      n_recommendations=n_recommendations)\n        print(\"... Done\")\n        return recommended\n\n    def _recommender(self):\n        return NearestNeighbors(metric=self.metric,\n                                algorithm=self.algorithm,\n                                n_neighbors=self.k,\n                                n_jobs=-1)\n\n    def _recommend(self, new_song, n_recommendations):\n        # Get the id of the recommended songs\n        recommendations = []\n        recommendation_ids = self._get_recommendations(\n            new_song=new_song, n_recommendations=n_recommendations)\n        # return the name of the song using a mapping dictionary\n        recommendations_map = self._map_indeces_to_song_title(\n            recommendation_ids)\n        # Translate this recommendations into the ranking of song titles recommended\n        for i, (idx, dist) in enumerate(recommendation_ids):\n            recommendations.append(recommendations_map[idx])\n        return recommendations\n\n    def _get_recommendations(self, new_song, n_recommendations):\n        # Get the id of the song according to the text\n        recom_song_id = self._fuzzy_matching(song=new_song)\n        # Start the recommendation process\n        print(f\"Starting the recommendation process for {new_song} ...\")\n        # Return the n neighbors for the song id\n        distances, indices = self.model.kneighbors(\n            self.data[recom_song_id], n_neighbors=n_recommendations + 1)\n        return sorted(list(\n            zip(indices.squeeze().tolist(),\n                distances.squeeze().tolist())),\n                      key=lambda x: x[1])[:0:-1]\n\n    def _map_indeces_to_song_title(self, recommendation_ids):\n        # get reverse mapper\n        return {\n            song_id: song_title\n            for song_title, song_id in self.decode_id_song.items()\n        }\n\n    def _fuzzy_matching(self, song):\n        match_tuple = []\n        # get match\n        for title, idx in self.decode_id_song.items():\n            ratio = fuzz.ratio(title.lower(), song.lower())\n            if ratio >= 60:\n                match_tuple.append((title, idx, ratio))\n        # sort\n        match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]\n        if not match_tuple:\n            print(\n                f\"The recommendation system could not find a match for {song}\")\n            return\n        return match_tuple[0][1]\n\n\nsong_info = pd.read_csv('../input/million-song-data-set-subset/10000.txt',\n                        sep='\\t',\n                        header=None)\nsong_info.columns = ['user_id', 'song_id', 'listen_count']\n\n#Read song  metadata\nsong_actual = pd.read_csv('../input/million-song-data-set-subset/song_data.csv')\nsong_actual.drop_duplicates(['song_id'], inplace=True)\n\n#Merge the two dataframes above to create input dataframe for recommender systems\nsongs = pd.merge(song_info, song_actual, on=\"song_id\", how=\"left\")\nsongs.to_csv('./songs.csv', index=False)\ndf_songs = pd.read_csv('./songs.csv')\n#dict1=dict()\n#for _,song_id,_,title,_,_,_ in songs.items():\n#    dict1[song_id]=title\n#print(songs.head())\nunique_songs = df_songs['title'].unique().shape[0]\nunique_artists = df_songs['artist_name'].unique().shape[0]\nunique_users = df_songs['user_id'].unique().shape[0]\n\n#Most popular songs\n\nten_pop_songs = df_songs.groupby(\n    'title')['listen_count'].count().reset_index().sort_values(\n        ['listen_count', 'title'], ascending=[0, 1])\nten_pop_songs['percentage'] = round(\n    ten_pop_songs['listen_count'].div(ten_pop_songs['listen_count'].sum()) *\n    100, 2)\n\nten_pop_songs = ten_pop_songs[:10]\n#print(ten_pop_songs)\nlabels = ten_pop_songs['title'].tolist()\ncounts = ten_pop_songs['listen_count'].tolist()\n\n#plt.figure()\n#sns.barplot(x=counts, y=labels, palette='Set3')\n#sns.despine(left=True, bottom=True)\n\n# Most popular artist\n\nten_pop_artists = df_songs.groupby(\n    ['artist_name'])['listen_count'].count().reset_index().sort_values(\n        ['listen_count', 'artist_name'], ascending=[0, 1])\n\nten_pop_artists = ten_pop_artists[:10]\n#print(ten_pop_artists)\n\n#plt.figure()\nlabels = ten_pop_artists['artist_name'].tolist()\ncounts = ten_pop_artists['listen_count'].tolist()\n#sns.barplot(x=counts, y=labels, palette='Set2')\n#sns.despine(left=True, bottom=True)\n\nlisten_counts = pd.DataFrame(df_songs.groupby('listen_count').size(),\n                             columns=['count'])\n\n#plt.figure(figsize=(20, 5))\n#sns.boxplot(x='listen_count', data=df_songs)\n#sns.despine()\nlisten_counts_temp = listen_counts[listen_counts['count'] > 50].reset_index(\n    drop=False)\n\n#plt.figure(figsize=(16, 8))\n#sns.barplot(x='listen_count',\n#            y='count',\n#            palette='Set3',\n#            data=listen_counts_temp)\n#plt.gca().spines['top'].set_visible(False)\n#plt.gca().spines['right'].set_visible(False)\n#plt.show()\nsong_user = df_songs.groupby('user_id')['song_id'].count()\n\n#plt.figure(figsize=(16, 8))\n#sns.distplot(song_user.values, color='orange')\n#plt.gca().spines['top'].set_visible(False)\n#plt.gca().spines['right'].set_visible(False)\n#plt.show()\n\nvalues_matrix = unique_users * unique_songs\n#Prepare the data\nsong_ten_id = song_user[song_user > 16].index.to_list()\n\ndf_song_id_more_ten = df_songs[df_songs['user_id'].isin(\n    song_ten_id)].reset_index(drop=True)\n\n# convert the dataframe into a pivot table\n\ndf_songs_features = df_song_id_more_ten.pivot(index='song_id',\n                                              columns='user_id',\n                                              values='listen_count').fillna(0)\n\n# obtain a sparse matrix\ndf=df_songs_features\ndf1=df.copy()\nmat_songs_features = csr_matrix(df_songs_features.values)\n\ndf_unique_songs = df_songs.drop_duplicates(subset=['song_id']).reset_index(\n    drop=True)[['song_id', 'title']]\n\ndecode_id_song = {\n    song: i\n    for i, song in enumerate(\n        list(\n            df_unique_songs.set_index('song_id').loc[\n                df_songs_features.index].title))\n}\n\nmodel = Recommender(metric='cosine',\n                    algorithm='brute',\n                    k=20,\n                    data=mat_songs_features,\n                    decode_id_song=decode_id_song)\n\nsong = 'shy Boy'\nnew_recommendations = model.make_recommendation(new_song=song,\n                                                n_recommendations=10)\nprint(f\"The recommendations for {song} are:\")\nprint(f\"{new_recommendations}\")\n\n\ndef recommend_songs(user, num_recommended_songs):\n\n    #print('The list of the songs {} Has Watched \\n'.format(user))\n\n    for m in df[df[user] > 0][user].index.tolist():\n        print(m)\n\n    print('\\n')\n\n    recommended_songs = []\n\n    for m in df[df[user] == 0].index.tolist():\n\n        index_df = df.index.tolist().index(m)\n        predicted_rating = df1.iloc[index_df, df1.columns.tolist().index(user)]\n        recommended_songs.append((m, predicted_rating))\n\n    sorted_rm = sorted(recommended_songs, key=lambda x: x[1], reverse=True)\n\n    print('The list of the Recommended songs \\n')\n    rank = 1\n    for recommended_song in sorted_rm[:num_recommended_songs]:\n\n        print('{}: {} - predicted count:{}'.format(rank, dict1[recommended_song[0]],\n                                                    recommended_song[1]))\n        rank = rank + 1\n\n\ndef song_recommender(user, num_neighbors, num_recommendation):\n\n    number_neighbors = num_neighbors\n\n    knn = NearestNeighbors(metric='cosine', algorithm='brute')\n    knn.fit(df.values)\n    distances, indices = knn.kneighbors(df.values,\n                                        n_neighbors=number_neighbors)\n\n    user_index = df.columns.tolist().index(user)\n\n    for m, t in list(enumerate(df.index)):\n        if df.iloc[m, user_index] == 0:\n            sim_songs = indices[m].tolist()\n            song_distances = distances[m].tolist()\n\n            if m in sim_songs:\n                id_song = sim_songs.index(m)\n                sim_songs.remove(m)\n                song_distances.pop(id_song)\n\n            else:\n                sim_songs = sim_songs[:n_neighbors - 1]\n                song_distances = song_distances[:n_neighbors - 1]\n\n            song_similarity = [1 - x for x in song_distances]\n            song_similarity_copy = song_similarity.copy()\n            nominator = 0\n\n            for s in range(0, len(song_similarity)):\n                if df.iloc[sim_songs[s], user_index] == 0:\n                    if len(song_similarity_copy) == (number_neighbors - 1):\n                        song_similarity_copy.pop(s)\n\n                    else:\n                        song_similarity_copy.pop(s -\n                                                 (len(song_similarity) -\n                                                  len(song_similarity_copy)))\n\n                else:\n                    nominator = nominator + song_similarity[s] * df.iloc[\n                        sim_songs[s], user_index]\n\n            if len(song_similarity_copy) > 0:\n                if sum(song_similarity_copy) > 0:\n                    predicted_r = nominator / sum(song_similarity_copy)\n\n                else:\n                    predicted_r = 0\n\n            else:\n                predicted_r = 0\n\n            df1.iloc[m, user_index] = predicted_r\n    recommend_songs(user, num_recommendation)\nsong_recommender('0012bf75d43a724f62dc746d9e85ae0088a3a1d6', 3, 4)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T05:24:14.010576Z","iopub.execute_input":"2021-11-20T05:24:14.010779Z","iopub.status.idle":"2021-11-20T05:24:27.738980Z","shell.execute_reply.started":"2021-11-20T05:24:14.010758Z","shell.execute_reply":"2021-11-20T05:24:27.738426Z"},"trusted":true},"execution_count":9,"outputs":[]}]}